\chapter{Related Work}
\label{ch:RelatedWork}

\section{Introduction}
\subsection{Basic Deep Learning Models}
The advent of deep learning brought about end-to-end approaches that learned features directly from raw data. Initial implementations utilised simple feed-forward Deep Neural Networks (DNNs) and Recurrent Neural Networks (RNNs) to model the temporal sequence of speech frames. While these models outperformed GMMs, they often suffered from the vanishing gradient problem when trained on long audio sequences, limiting their depth and capacity.

\section{Deep Residual Learning in Audio}
To overcome the limitations of standard deep networks, \cite{he2016deep} introduced Residual Networks (ResNets). The core innovation was the residual block, which utilizes skip connections to allow gradients to flow through the network unimpeded during backpropagation (see Figure \ref{fig:residual_block}).
\begin{figure}[h]
    \centering
    % Placeholder for a standard ResNet block diagram if the user had one.
    \caption{Conceptual diagram of a Residual Building Block \cite{he2016deep}.}
    \label{fig:residual_block}
\end{figure}

Although originally designed for ImageNet classification, ResNets have proven highly effective for audio tasks when audio is treated as an image (spectrogram). \cite{todisco2019asvspoof} demonstrated that ResNet-based architectures could serve as powerful baselines for the ASVSpoof 2019 challenge. By converting raw audio waveforms into log-mel spectrograms, these models can leverage the 2D translational invariance of Convolutional Neural Networks (CNNs) to detect local spectral artifacts indicative of spoofing.

The advantages of ResNets in this domain are twofold:
\begin{enumerate}
    \item \textbf{Depth:} They allow for significantly deeper architectures (e.g., ResNet-34, ResNet-50) without performance degradation.
    \item \textbf{Feature Hierarchy:} They learn a hierarchy of features, from low-level edges in the spectrogram (formants) to high-level abstract representations of "naturalness."
\end{enumerate}

\section{Adversarial Attacks on Machine Learning}
While deep learning models have achieved super-human performance in many tasks, they exhibit a counter-intuitive brittleness. \cite{szegedy2013intriguing} first discovered "adversarial examples"---inputs formed by applying small, intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence.

\cite{goodfellow2014explaining} formalized this with the Fast Gradient Sign Method (FGSM), positing that the vulnerability arises from the linear nature of deep networks in high-dimensional spaces. \cite{madry2017towards} later introduced Projected Gradient Descent (PGD), an iterative refinement of FGSM, establishing it as the standard benchmark for evaluating adversarial robustness.

\section{Adversarial Audio}
The application of adversarial attacks to the audio domain presents unique challenges compared to images. Audio signals are temporal and often require pre-processing (like STFT) which can be non-differentiable or difficult to invert. However, recent works by \cite{carlini2018audio} and \cite{alzantot2018did} have successfully demonstrated targeted attacks against Automatic Speech Recognition (ASR) systems.

In the context of ASV, \cite{das2020attacker} highlighted that attacker models could bypass state-of-the-art verification systems. The ASVSpoof 2019 baseline models, despite their robustness to diverse voice conversion algorithms, have shown susceptibility to these gradient-based perturbations. This creates a "cat-and-mouse" dynamic where defenders must essentially train against every conceivable attack strategy, a requirement known as adversarial training.

\section{Summary}
The literature establishes that while ResNets are the current state-of-the-art for ASV spoofing detection, their linearity makes them inherently vulnerable to gradient-based attacks. The intersection of these two fields---ResNet-based audio forensics and adversarial machine learning---is the focal point of this research project.
