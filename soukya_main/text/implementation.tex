\chapter{Implementation}
\label{ch:Implementation}

\section{Experimental Environment}
The experiments presented in this dissertation were computationally intensive, requiring hardware acceleration for efficient training of deep convolutional networks.

\subsection{Hardware Infrastructure}
All model training and evaluation were performed on the Google Colab Pro+ platform, which provides access to high-performance GPUs. The specific specifications of the environment are as follows:
\begin{itemize}
    \item \textbf{GPU:} NVIDIA Tesla T4 or P100 with 16GB GDDR6 VRAM. The GPU was utilized for all tensor operations, forward passes, and backpropagation.
    \item \textbf{CPU:} Intel Xeon Processor @ 2.20GHz (2 vCPUs), responsible for data loading and preprocessing (so as not to bottleneck the GPU).
    \item \textbf{RAM:} 25GB System RAM, ensuring the entire dataset metadata could be loaded into memory.
    \item \textbf{Storage:} The dataset (approx. 20GB) was hosted on Google Drive and accessed via a FUSE mount (`google-drive-ocamlfuse`) to minimize I/O latency.
\end{itemize}

\subsection{Software Stack}
The implementation leveraged the Python scientific computing ecosystem. Key libraries include:
\begin{itemize}
    \item \textbf{PyTorch (v1.9+):} Served as the primary Deep Learning framework. Its dynamic computational graph was crucial for implementing the iterative PGD attacks.
    \item \textbf{Torchaudio:} Used for efficient audio loading and STFT/Mel-spectrogram computations directly on the GPU.
    \item \textbf{Numpy \& Pandas:} For numerical operations and managing the protocol (label) files.
    \item \textbf{Scikit-learn:} Provided utility functions for calculating metrics like ROC-AUC, F1-Score, and creating confusion matrices.
    \item \textbf{Matplotlib \& Seaborn:} Employed for generating high-quality visualizations of the training curves and confusion matrices.
\end{itemize}

\section{Dataset Preparation}
Experiments were conducted on the Logical Access (LA) partition of the ASVspoof 2019 dataset \cite{wang2020asvspoof}.

\subsection{Train/Dev/Eval Split}
The dataset is officially partitioned into three subsets:
\begin{enumerate}
    \item \textbf{Training Set (Train):} Comprises 2,580 bona fide and 22,800 spoofed utterances from 20 speakers. This set is used strictly for gradient updates.
    \item \textbf{Development Set (Dev):} Contains 2,548 bona fide and 22,296 spoofed utterances from a different set of 20 speakers (unseen during training). This set is used for hyperparameter tuning, model checkpointing, and Early Stopping.
    \item \textbf{Evaluation Set (Eval):} A held-out set containing attacks generated by algorithms unknown to the developers (A17-A19). This measures the system's generalization to unseen attacks (Zero-Shot Learning).
\end{enumerate}

\subsection{Class Imbalance Strategy}
The dataset exhibits a significant class imbalance (approx. 1:10 Bona fide to Spoof ratio). Training on raw data would bias the model towards predicting "Spoof" to minimize loss. To mitigate this, we implemented a \texttt{WeightedRandomSampler} in PyTorch. Weights were assigned to each sample inversely proportional to its class frequency:
\begin{equation}
    W_{class} = \frac{N_{total}}{N_{class}}
\end{equation}
This ensures that each training batch contains a roughly 50/50 split of real and spoofed audio, stabilizing the Cross-Entropy loss.

\section{Training Protocols}
We established three distinct training protocols to evaluate the impact of adversarial hardening.

\subsection{Protocol A: Baseline Training}
The baseline model was trained exclusively on clean, unperturbed data.
\begin{itemize}
    \item \textbf{Optimization:} The Adam optimizer was used with an initial learning rate of $1e-4$ and weight decay of $1e-4$ (L2 regularization).
    \item \textbf{Scheduling:} A \texttt{ReduceLROnPlateau} scheduler reduced the learning rate by a factor of 0.1 if validation loss plateaued for 5 epochs.
    \item \textbf{Criterion:} Binary Cross Entropy Loss was minimized.
\end{itemize}

\subsection{Protocol B: Adversarial Training (FGSM)}
In this protocol, for every batch of data $(x, y)$, we generated adversarial examples $x_{adv}$ using FGSM with $\epsilon=0.03$. The model was then updated on the adversarial examples. This forces the model to learn features that are invariant to single-step perturbations.

\subsection{Protocol C: Adversarial Training (PGD)}
This represents the most rigorous training regime. For each batch, we generated $x_{adv}$ using PGD with 7 iterations ($\alpha=0.01, \epsilon=0.03$). Training on PGD examples is computationally expensive (approx. 7$\times$ slower than baseline) but is theoretically guaranteed to solve the inner maximization problem of the min-max adversarial game.

All models were trained for 50 epochs with Early Stopping set to a patience of 10 epochs to prevent overfitting.
