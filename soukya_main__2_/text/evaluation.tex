\section{Evaluation}
\label{sec:Evaluation}
This section presents a comprehensive evaluation of the proposed adversarial robustness framework for Automatic Speaker Verification (ASV) systems. The experiments were conducted using the Logical Access (LA) partition of the ASVspoof 2019 dataset, comparing the performance of a baseline ResNet-18 model against models trained with Adversarial Training using PGD and FGSM attacks.

\subsection{Experimental Results}

\subsubsection{Quantitative Analysis}
Table \ref{tab:baseline_performance} summarizes the performance of the Baseline ResNet-18 model on the standard ASVspoof 2019 Evaluation set (clean data). The model achieves state-of-the-art performance with an Equal Error Rate (EER) of just 0.52\%.

\begin{table}[h]
    \centering
    \caption{Baseline Model Performance on Clean Test Data}
    \label{tab:baseline_performance}
    \begin{tabular}{|l|c|}
        \hline
        \textbf{Metric} & \textbf{Value} \\
        \hline
        Accuracy & 99.42\% \\
        Precision & 99.35\% \\
        Recall & 99.51\% \\
        F1-Score & 99.43\% \\
        \textbf{Equal Error Rate (EER)} & \textbf{0.52\%} \\
        \hline
    \end{tabular}
\end{table}

However, when subjected to adversarial attacks, the performance degrades catastrophically (Table \ref{tab:attack_impact}). The FGSM attack raises the EER to over 92\%, while the PGD attack renders the system completely ineffective with an EER of 98.7\%.

\begin{table}[h]
    \centering
    \caption{Impact of Adversarial Attacks on Unsecured Baseline Model}
    \label{tab:attack_impact}
    \begin{tabular}{|l|c|c|}
        \hline
        \textbf{Attack Type} & \textbf{Accuracy} & \textbf{EER} \\
        \hline
        No Attack (Clean) & 99.42\% & 0.52\% \\
        FGSM ($\epsilon=0.03$) & 8.50\% & 92.10\% \\
        PGD ($\epsilon=0.03, \alpha=0.01$) & 1.20\% & 98.70\% \\
        \hline
    \end{tabular}
\end{table}

\subsubsection{Robustness Evaluation}
Implementing Adversarial Training significantly recovers performance. Table \ref{tab:robustness} compares the robustness of models trained with FGSM and PGD adversarial examples. The PGD-trained model exhibits superior resilience, maintaining an EER of 2.8\% even under a strong PGD attack.

\begin{table}[h]
    \centering
    \caption{Comparison of Adversarially Trained Models (Robustness)}
    \label{tab:robustness}
    \begin{tabular}{|l|c|c|c|}
        \hline
        \textbf{Training Method} & \textbf{Clean EER} & \textbf{FGSM Attack EER} & \textbf{PGD Attack EER} \\
        \hline
        Baseline & \textbf{0.52\%} & 92.1\% & 98.70\% \\
        FGSM Training & 2.10\% & 5.40\% & 12.30\% \\
        PGD Training & 2.45\% & \textbf{3.10\%} & \textbf{2.80\%} \\
        \hline
    \end{tabular}
\end{table}

\subsection{Confusion Matrix Analysis}
To visualize the classification boundaries, we present the confusion matrices for the most critical scenarios.

\subsubsection{Baseline Performance}
The baseline model's near-perfect separation of classes on clean data is visible in Figure \ref{fig:cm_baseline}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/confusion_matrix_baseline.png}
    \caption{Confusion Matrix of the Baseline Model on Clean Data.}
    \label{fig:cm_baseline}
\end{figure}

\subsubsection{Adversarial Robustness}
Figures \ref{fig:cm_pgd} and \ref{fig:cm_fgsm} illustrate the recovered performance of the hardened models. The PGD model (Figure \ref{fig:cm_pgd}) successfully re-classifies the majority of adversarial spoof attempts as ``Spoof'', whereas the standard baseline would have classified them as ``Bonafide''.

\begin{figure}[h]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/confusion_matrix_pgd.png}
        \caption{Confusion Matrix: PGD Model under PGD Attack.}
        \label{fig:cm_pgd}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/confusion_matrix_pgd_wild.png}
        \caption{Confusion Matrix: PGD Model on Wild Dataset.}
        \label{fig:cm_pgd_wild}
    \end{minipage}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/confusion_matrix_fgsm.png}
        \caption{Confusion Matrix: FGSM Model under FGSM Attack.}
        \label{fig:cm_fgsm}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/confusion_matrix_fgsm_wild.png}
        \caption{Confusion Matrix: FGSM Model on Wild Dataset.}
        \label{fig:cm_fgsm_wild}
    \end{minipage}
\end{figure}

\subsection{Summary Comparison}
Figure \ref{fig:comparison} provides a visual summary of the trade-off between clean accuracy and adversarial robustness. While adversarial training introduces a slight drop in clean accuracy (from 99.4\% to ~97.5\%), it prevents total system failure under attack.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/comparison.png}
    \caption{Comparative Performance: Baseline vs. PGD vs. FGSM Adversarial Training.}
    \label{fig:comparison}
\end{figure}

\subsection{Summary}
The evaluation demonstrates that while the baseline ResNet-18 achieves excellent clean accuracy, it collapses under adversarial perturbations. FGSM training improves resilience moderately, but PGD training provides the strongest defense, balancing clean performance with robustness against gradient-based attacks.

\section{Conclusion}
The experimental results confirm that relying solely on clean accuracy is dangerous for security systems. While the Baseline model appears superior with 0.52\% EER, it is defenseless against gradient attacks (98.7\% EER). PGD Adversarial Training provides the most robust defense, reducing the worst-case EER to 2.80\%, making it the recommended strategy for deployment.
