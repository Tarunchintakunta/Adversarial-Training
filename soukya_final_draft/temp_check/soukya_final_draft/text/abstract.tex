\begin{abstract}
The security of Automatic Speaker Verification (ASV) systems is increasingly challenged by sophisticated spoofing attacks, including Text-to-Speech (TTS) synthesis and Voice Conversion (VC). While deep learning, particularly Residual Networks (ResNets), has set new benchmarks for detection accuracy on datasets like ASVSpoof 2019, the vulnerability of these models to adversarial perturbations remains a critical concern. This research investigates the robustness of a ResNet-18 based spoofing detector against two prominent adversarial attacks: the Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD).

We implement an end-to-end detection pipeline that processes raw audio into log-mel spectrograms, which serve as inputs to the ResNet visual classifier. Our experimental results on the ASVSpoof 2019 Logical Access (LA) partition demonstrate that while the baseline model achieves high detection accuracy (approx. 90\%), its performance is fragile when subjected to adversarial noise. Specifically, we observe that FGSM attacks cause a catastrophic degradation in the Equal Error Rate (EER), pushing it above 90\%, whereas PGD attacks, despite being iteratively stronger in theory, show a varied impact depending on perturbation magnitude. This study provides a comprehensive analysis of the trade-offs between model complexity and adversarial robustness, underscoring the urgent need for adversarial training and robust feature extraction in deployment-ready biometric security systems.
\end{abstract}
