\section{Introduction}
\label{ch:Introduction}

\subsection{Background and Motivation}
In an era where biometric authentication is becoming the standard for securing personal and financial assets, Automatic Speaker Verification (ASV) systems have seen widespread adoption. From banking voice assistants to smart home devices, the ability to verify identity through voice offers convenience and security. However, this proliferation has been accompanied by an escalation in security threats. Malicious actors now have access to advanced deep learning tools capable of generating highly realistic synthetic speech (Text-to-Speech or TTS) and cloning voices (Voice Conversion or VC) with minimal training data.

The vulnerability of ASV systems to these ``spoofing'' or ``presentation'' attacks has catalyzed the development of Presentation Attack Detection (PAD) systems. The ASVSpoof challenge series \cite{wu2015asvspoof, wang2020asvspoof} has been instrumental in benchmarking these countermeasures. While significant progress has been made using Deep Neural Networks (DNNs) like Convolutional Neural Networks (CNNs) and Residual Networks (ResNets) \cite{he2016deep}, a new vector of threat has emerged: adversarial attacks.

Adversarial attacks involve adding imperceptible, structured noise to an input signal---in this case, audio---to mislead a neural network classifier. Unlike standard spoofing attacks which attempt to sound like the target speaker to a human, adversarial examples are optimized to exploit the mathematical vulnerabilities of the model itself. Research by \cite{goodfellow2014explaining} and \cite{madry2017towards} in the image domain has shown that even state-of-the-art models can be easily fooled. This project explores whether these vulnerabilities translate to the audio domain in the context of ASV spoofing detection.

\subsection{Problem Statement}
Current state-of-the-art audio spoofing detectors achieve low Equal Error Rates (EER) on standard benchmarks like ASVSpoof 2019. However, their reliability in adversarial settings is questionable. A system that is 99\% accurate on standard spoofing attacks may fail completely when a sophisticated attacker applies a gradient-based perturbation like Projected Gradient Descent (PGD). There is a lack of comprehensive studies that quantify this specific vulnerability for ResNet-based architectures within the ASVSpoof ecosystem. This research addresses this gap by rigorously evaluating the resilience of ResNet-18 against both single-step (FGSM) and iterative (PGD) adversarial attacks.

\subsection{Research Aim and Objectives}
The primary aim of this project is to critically evaluate the robustness of deep residual learning for audio spoofing detection. The specific objectives are:
\begin{itemize}
    \item \textbf{Implementation:} Develop a complete spoofing detection pipeline using ResNet-18, including preprocessing steps for converting raw audio to mel-spectrograms.
    \item \textbf{Baseline Evaluation:} Benchmark the model's performance on the ASVSpoof 2019 Logical Access dataset using standard metrics (EER, F1-Score).
    \item \textbf{Adversarial Simulation:} Implement and simulate Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD) attacks to generate adversarial audio examples.
    \item \textbf{Robustness Analysis:} Quantify the degradation in model performance under attack and analyze the relationship between perturbation magnitude and detection failure.
\end{itemize}

\subsection{Scope and Limitations}
This study focuses on the Logical Access (LA) scenario of the ASVSpoof 2019 challenge, limiting the scope to TTS and VC attacks. Physical Access (PA) attacks (replay) are excluded. Furthermore, the adversarial attacks are generated in a ``white-box'' setting, where the attacker is assumed to have full access to the model's gradients. This represents a worst-case security scenario.

\subsection{Report Structure}
This report is structured as follows:
\begin{itemize}
    \item \textbf{Chapter \ref{ch:RelatedWork}: Literature Review} surveys the evolution of ASV systems, spoofing countermeasures, and the theory of adversarial machine learning.
    \item \textbf{Chapter \ref{ch:Methodology}: Methodology} details the theoretical framework of ResNets and the mathematical formulation of the adversarial attacks used.
    \item \textbf{Chapter \ref{ch:Implementation}: Design and Implementation} describes the experimental setup, software stack, and training protocols.
    \item \textbf{Chapter \ref{ch:Evaluation}: Evaluation and Results} presents the experimental data, analysis of results, and visualisations including confusion matrices and ROC curves.
    \item \textbf{Chapter \ref{ch:Conclusion}: Conclusion} summarizes the key findings and suggests directions for future research in robust audio forensics.
\end{itemize}
